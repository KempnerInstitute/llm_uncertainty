import os
from pathlib import Path
import random
from typing import Optional

import lightning as L
import torch
import torch.nn as nn

import matplotlib.pyplot as plt
from train_head_utils import load_lm_head
from repetition import compute_entropy
from transformers import (
    AutoTokenizer,
)
from lit_llama import Tokenizer


DEVICE="cuda"
# DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32
DTYPE = torch.float32


def main(
    *,
    repetition_dir: str, 
    model_type: str,
    model_size: str,
    checkpoint_path: str,
    tokenizer_path: Optional[str] = None, 
    output_dir: Optional[str] = None,
    seed: int = 42,
) -> None:
    """
    Args:
        repetition_dir: director to find repetition.py (generated by precompute_logits.py)
        output_dir: Where to save output files
        checkpoint_path: The small LM checkpoint path.
        batch_size: Batch size.
    """
    # MUST COME FIRST
    args = locals()

    torch.manual_seed(seed)
    random.seed(seed)

    assert checkpoint_path.exists()

    if(model_type == "llama"):
        assert(tokenizer_path is not None)

    # load lm head
    lm_head = load_lm_head(
        checkpoint_path, dtype=DTYPE, device=DEVICE, model_type=model_type, model_size=model_size
    )
    print("small lm head loaded")

    # load tokenizer
    global tokenizer 
    if model_type == "pythia":
        tokenizer= AutoTokenizer.from_pretrained(
            f"EleutherAI/pythia-{model_size}",
        )
    elif model_type == "llama": 
        tokenizer = Tokenizer(tokenizer_path)

    repetition_shards = os.listdir(repetition_dir)
    repetition_shards = list(sorted(repetition_shards, key=lambda x: int(x.split('_')[-1].strip('.pt'))))
    print(repetition_shards)

    global encoded_prompts, large_entropy, original_prediction

    new_embed_all = []
    original_embed_all = []
    large_entropy = []
    prompt_type = []
    encoded_prompts = []
    for repetition_shard in repetition_shards:
        shard_path = os.path.join(repetition_dir, repetition_shard)
        data = torch.load(shard_path, map_location=DEVICE)
    
        new_embed_all.append(data["new_embed"].to(DTYPE))
        original_embed_all.append(torch.stack(data["original_embed"]).to(DTYPE))
        n_samples = data["new_embed"].shape[0]
        large_entropy.append(torch.concatenate(data["large_entropy"][0: n_samples]).cpu())
        prompt_type.append((data["prompt_type"][0: n_samples]).bool().cpu())
        encoded_prompts += data["encoded_prompt"]

    new_embed_all = torch.concatenate(new_embed_all)
    original_embed_all = torch.concatenate(original_embed_all)
    large_entropy = torch.concatenate(large_entropy)
    prompt_type = torch.concatenate(prompt_type)
    # print("shape checks: ", new_embed_all.shape, original_embed_all.shape, large_entropy.shape, prompt_type.shape, len(encoded_prompts))
    print("shape checks: ", new_embed_all.shape, original_embed_all.shape, large_entropy.shape, prompt_type.shape)

    original_logits = []
    new_logits = []
    for i in range(len(original_embed_all)):
        original_logits.append(lm_head(original_embed_all[i]).detach())
    original_logits = torch.stack(original_logits)

    new_logits = lm_head(new_embed_all).detach()
    print("logits shape: ", original_logits.shape)

    original_probs = torch.softmax(original_logits, dim=-1)
    new_probs = torch.softmax(new_logits, dim=-1)

    original_entropy = compute_entropy(original_logits.cpu())
    new_entropy = compute_entropy(new_logits.cpu())

    original_prediction = torch.argmax(original_probs, dim=-1)

    global img_output_dir
    if output_dir is not None:
        img_output_dir = output_dir
    else: 
        img_output_dir = os.path.join(os.path.dirname(repetition_dir), "results")
        Path(img_output_dir).mkdir(parents=True, exist_ok=True)


    # plot_entropy(original_entropy.numpy(), new_entropy.numpy(), large_entropy.numpy(), prompt_type.numpy())

    mutual_information(original_probs, new_probs, prompt_type)

    # probs_norm(original_probs, new_probs, prompt_type)

    # kl_divergence(original_probs, new_probs, prompt_type)

    # weighted_entropy(original_entropy, new_entropy, original_probs, prompt_type)

    return


def weighted_entropy(original_entropy, new_entropy, original_probs, prompt_type):
    px = torch.topk(original_probs, 10, dim=1)
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True)).cpu()

    weighted_entropy = (new_entropy * p_x).sum(dim=1)
    print("weighted entropy:", weighted_entropy.shape, (weighted_entropy<1.47).sum())

    plt.figure()
    plt.hist(weighted_entropy[prompt_type].cpu().numpy(), bins=30, histtype="step", label="low_e_high_a")
    plt.hist(weighted_entropy[~prompt_type].cpu().numpy(), bins=30, histtype="step", label="high_e_low_a")
    plt.xlabel("Weighted Entropy")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/weighted_entropy.png", bbox_inches='tight')

    print("--- weighted entropy classifier ---")
    simple_classification(weighted_entropy.to(DEVICE), prompt_type.to(DEVICE))

    return 


def probs_norm(x_probs, y_probs, prompt_type):
    px = torch.topk(x_probs, 10, dim=1)
    top_k_idx = px.indices
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True))

    p_y_given_x = torch.empty_like(p_x)
    for i in range(top_k_idx.shape[0]):
        for j in range(top_k_idx.shape[1]):
            ii_idx = top_k_idx[i, j]
            p_y_given_x[i, j] = y_probs[i, j, ii_idx]

    norm = torch.sum(p_x * p_y_given_x, dim=1)

    plt.figure()
    plt.hist(norm[prompt_type].cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(norm[~prompt_type].cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Probability Norm")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/probs_norm.png", bbox_inches='tight')

    return


def kl_divergence(x_probs, y_probs, prompt_type):
    px = torch.topk(x_probs, 10, dim=1)
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True))

    p_y_given_x = y_probs

    p_xy = torch.mul(p_x.unsqueeze(-1), p_y_given_x)
    
    p_y = torch.sum(p_xy, dim=1)

    total_m = 0.5 * (x_probs + p_y)
    kl = nn.functional.kl_div(total_m, x_probs, reduction="none", log_target=True)
    kl += nn.functional.kl_div(total_m, p_y, reduction="none", log_target=True)
    kl = 0.5 * kl.sum(dim=-1)

    plt.figure()
    plt.hist(kl[prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(kl[~prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("KL Divergence")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/kl_div.png", bbox_inches='tight')
    
    print("--- JSD classifier ---")
    simple_classification(kl.to(DEVICE), prompt_type.to(DEVICE))

    return


def mutual_information(x_probs, y_probs, prompt_type):
    px = torch.topk(x_probs, 10, dim=1)
    top_k_idx = px.indices
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True))

    print(top_k_idx.shape, y_probs.shape)

    # if choose to restrict the support of Y to only top K tokens
    p_y_given_x = torch.empty(p_x.shape[0], p_x.shape[1], p_x.shape[1], device=p_x.device)
    for i in range(top_k_idx.shape[0]):
        ii_idx = top_k_idx[i]
        for j in range(top_k_idx.shape[1]):
            p_y_given_x[i, j, :] = y_probs[i, j, ii_idx]
    p_y_given_x = torch.div(p_y_given_x, p_y_given_x.sum(dim=2, keepdim=True))
    
    # if choose support of Y to be all tokens
    # p_y_given_x = y_probs

    p_xy = torch.mul(p_x.unsqueeze(-1), p_y_given_x)
    
    HYX = -torch.sum(p_xy * p_y_given_x.log(), dim=[1, 2])

    p_y = torch.sum(p_xy, dim=1)
    HY = -torch.sum(p_y * p_y.log(), dim=1)

    MI = HY - HYX

    plt.figure(figsize=(12, 12))
    plt.subplot(3, 2, 1)
    plt.hist(HY[prompt_type].cpu().numpy(), bins=30, histtype="step", label="low_e_high_a")
    plt.hist(HY[~prompt_type].cpu().numpy(), bins=30, histtype="step", label="high_e_low_a")
    plt.xlabel("H(Y)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 2)
    plt.hist(HYX[prompt_type].cpu().numpy(), bins=30, histtype="step", label="low_e_high_a")
    plt.hist(HYX[~prompt_type].cpu().numpy(), bins=30, histtype="step", label="high_e_low_a")
    plt.xlabel("H(Y|X)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 3)
    plt.hist(MI[prompt_type].cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(MI[~prompt_type].cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Mutual Information I(X;Y)")
    plt.ylabel("Count")
    plt.legend()


    plt.subplot(3, 2, 4)
    plt.hist(MI[prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(MI[~prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Log(Mutual Information)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 5)
    plt.hist((MI/HY)[prompt_type].cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist((MI/HY)[~prompt_type].cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Normalized Mutual Information I(X;Y) / H(Y)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 6)
    plt.hist((MI/HY)[prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist((MI/HY)[~prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("log(Normalized Mutual Information)")
    plt.ylabel("Count")
    plt.legend()

    plt.savefig(f"{img_output_dir}/mutual_info.png", bbox_inches='tight')
    
    print("--- MI classifier ---")
    simple_classification(MI.to(DEVICE), prompt_type.to(DEVICE))

    print("--- log(MI) classifier ---")
    simple_classification((MI).log().to(DEVICE), prompt_type.to(DEVICE))

    print("--- MI_normal classifier ---")
    simple_classification((MI/HY).to(DEVICE), prompt_type.to(DEVICE))

    print("--- log(MI_normal) classifier ---")
    simple_classification((MI/HY).log().to(DEVICE), prompt_type.to(DEVICE))

    return HYX, HY, MI 


def plot_entropy(original_entropy, new_entropy, large_entropy, prompt_type):

    plt.figure()
    plt.hist(new_entropy[prompt_type].flatten(), bins=50, histtype="step", label="ALL(new low_e_high_a) - orig")
    plt.hist(new_entropy[~prompt_type].flatten(), bins=50, histtype="step", label="ALL(new high_e_low_a) - orig")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/new_entropy.png", bbox_inches='tight')

    plt.figure(figsize=(12, 12))
    plt.subplot(2, 2, 1)
    plt.hist(original_entropy[prompt_type].flatten() - new_entropy[prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new low_e_high_a)  - orig")
    plt.hist(original_entropy[~prompt_type].flatten() - new_entropy[~prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new high_e_low_a) - orig")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.title("A")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(2, 2, 2)
    plt.hist(original_entropy[prompt_type].flatten() - new_entropy[prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new low_e_high_a)  - orig")
    plt.hist(original_entropy[~prompt_type].flatten() - new_entropy[~prompt_type].max(axis=1), bins=50, histtype="step", label="MAX(new high_e_low_a  - orig)")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.title("B")
    plt.legend()


    plt.subplot(2, 2, 3)
    plt.hist(original_entropy[prompt_type].flatten() - new_entropy[prompt_type].max(axis=1), bins=50, histtype="step", label="MAX(new low_e_high_a) - orig")
    plt.hist(original_entropy[~prompt_type].flatten() - new_entropy[~prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new high_e_low_a) - orig")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.title("C")
    plt.legend()

    plt.subplot(2, 2, 4)
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.title("D")
    plt.legend()

    plt.savefig(f"{img_output_dir}/new_entropy_breakdown.png", bbox_inches='tight')


    plt.figure()
    plt.hist(large_entropy, bins=30, histtype="step", label="Large Model Original Entropy")
    plt.hist(original_entropy[prompt_type], bins=30, histtype="step", label="Small Model Original Entropy low_e_high_a")
    plt.hist(original_entropy[~prompt_type], bins=30, histtype="step", label="Small Model Original Entropy shigh_e_low_a")
    plt.xlabel("Original Entropy")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/original_entropy.png")

    print("--- original entropy classifier ---")
    simple_classification(torch.from_numpy(original_entropy).to(DEVICE), torch.from_numpy(prompt_type).to(DEVICE))

    # print("--- ALL(entropy) classifier ---")
    # simple_classification(torch.from_numpy(new_entropy).to(DEVICE), torch.from_numpy(prompt_type).to(DEVICE))


def simple_classification(x_train, y_train):
    if len(x_train.shape) == 1:
        x_train = x_train.reshape(-1, 1)
    class LinearClassifier(torch.nn.Module):
        def __init__(self, input_dim=1, output_dim=1):
            super(LinearClassifier, self).__init__()
            self.linear = torch.nn.Linear(input_dim, output_dim)
            # self.bias = torch.nn.Parameter(torch.rand(1,1))

        def forward(self, x):
            x = self.linear(x)
            # x = x + self.bias
            return x
        
    model = LinearClassifier(input_dim=x_train.shape[1])
    model.to(DEVICE)
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.2)

    all_loss = []
    for i in range(2001):
        output = model(x_train)

        loss = criterion(output.view(-1), y_train.float())
        all_loss.append(loss.item())
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()

        if i % 200 == 0: 
            prediction = output.detach().view(-1) > 0
            accuracy = torch.sum((prediction==y_train).bool()) / y_train.shape[0]
            print(f"\tEpoch {i}, Loss:{loss.item():.3f}, Acc:{accuracy.item():.2f}")

    # output = output.view(-1).detach()
    # # most wrong examples
    # for type in [0, 1]:
    #     wrong_idx = torch.argwhere((prediction!=y_train)&(y_train == type)).view(-1)
    #     wrong_idx_topk = torch.topk(output[wrong_idx], k=20, largest=not bool(type)).indices
    #     wrong_idx = wrong_idx[wrong_idx_topk]
    #     for p in wrong_idx:
    #         if type == 0:
    #             print(f"\n************ example: true type=epistemic, predicted type=aleatoric ************\n")
    #         elif type == 1:
    #             print(f"\n************ example: true type=aleatoric, predicted type=epistemic ************\n")
    #         decoded_prompt = tokenizer.decode(encoded_prompts[p].squeeze())
    #         decoded_prediction = tokenizer.decode(original_prediction[p])
    #         print(f"{decoded_prompt} [{decoded_prediction} (H={large_entropy[p]:.2f})]")

    # # most correct  examples
    # for type in [0, 1]:
    #     wrong_idx = torch.argwhere((prediction==y_train)&(y_train == type)).view(-1)
    #     wrong_idx_topk = torch.topk(output[wrong_idx], k=20, largest=bool(type)).indices
    #     wrong_idx = wrong_idx[wrong_idx_topk]
    #     for p in wrong_idx:
    #         if type == 0:
    #             print(f"\n************ example: true type=epistemic, predicted type=epistemic ************\n")
    #         elif type == 1:
    #             print(f"\n************ example: true type=aleatoric, predicted type=aleatoric ************\n")

    #         decoded_prompt = tokenizer.decode(encoded_prompts[p].squeeze())
    #         decoded_prediction = tokenizer.decode(original_prediction[p])
    #         print(f"{decoded_prompt} [{decoded_prediction} (H={large_entropy[p]:.2f})]")


    return


    
if __name__ == "__main__":

    from jsonargparse import CLI

    CLI(main)
